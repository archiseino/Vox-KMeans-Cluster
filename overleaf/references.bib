@online{PasalDemokrasi,
    author    = {Kumparan},
    title     = {Isi Pasal 1 Ayat 2 UUD1945 Sebelum dan Sesudah Amandemen},
    date      = {Accessed 2024},
    url       = {https://kumparan.com/berita-terkini/isi-pasal-1-ayat-2-uud1945-sebelum-dan-sesudah-amandemen-1weOIUtUKWB},
}

@online{VisiMisiKPU,
    author    = {KPU},
    title     = {Visi dan Misi KPU},
    date      = {Accessed 2024},
    url       ={https://www.kpu.go.id/page/read/4/visi-dan-misi},
}

@online{DPTPilkada,
    author    = {KPU},
    title     = {DptPilkada},
    date      = {Accessed 2024},
    url       ={https://www.kpu.go.id/berita/baca/11702/dpt-pemilu-2024-nasional-2048-juta-pemilih
},
}

@online{InstrumenDpt,
    author    = {Komisi Pemilihan Umum},
    title     = {Pentingnya Daftar Pemilih Tetap yang Valid untuk Pemilu yang Demokratis},
    date      = {Accessed 2024},
    url       = {kpu.go.id},
}

@article{ImplementasiDataPemilihBerkelanjutan,
author = {Ointu, Lanny and Rotty, Viktory and Mamonto, Fitri},
year = {2022},
month = {11},
pages = {2969-2976},
title = {IMPLEMENTASI PROGRAM PEMUTAKHIRAN DATA PEMILIH BERKELANJUTAN DI KOTA MANADO},
volume = {1},
journal = {SIBATIK JOURNAL: Jurnal Ilmiah Bidang Sosial, Ekonomi, Budaya, Teknologi, dan Pendidikan},
doi = {10.54443/sibatik.v1i12.478}
}

@inproceedings{AssociationDataMining,
author = {Liu, Bing and Hsu, Wynne and Ma, Yiming},
title = {Integrating classification and association rule mining},
year = {1998},
publisher = {AAAI Press},
abstract = {Classification rule mining aims to discover a small set of rules in the database that forms an accurate classifier. Association rule mining finds all the rules existing in the database that satisfy some minimum support and minimum confidence constraints. For association rule mining, the target of discovery is not pre-determined, while for classification rule mining there is one and only one predetermined target. In this paper, we propose to integrate these two mining techniques. The integration is done by focusing on mining a special subset of association rules, called class association rules (CARs). An efficient algorithm is also given for building a classifier based on the set of discovered CARs. Experimental results show that the classifier built this way is, in general, more accurate than that produced by the state-of-the-art classification system C4.5. In addition, this integration helps to solve a number of problems that exist in the current classification systems.},
booktitle = {Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining},
pages = {80–86},
numpages = {7},
location = {New York, NY},
series = {KDD'98}
}

@article{DataMiningTechniques,
author = {Han, Jiawei and Kamber, M.},
year = {2006},
month = {01},
pages = {},
title = {Data mining: concepts and techniques morgan kaufmann},
volume = {54}
}

@INPROCEEDINGS{DbScanClustering,
  author={Deng, Dingsheng},
  booktitle={2020 7th International Forum on Electrical Engineering and Automation (IFEEA)}, 
  title={DBSCAN Clustering Algorithm Based on Density}, 
  year={2020},
  volume={},
  number={},
  pages={949-953},
  keywords={Machine learning algorithms;Clustering algorithms;Machine learning;Big Data;Prediction algorithms;Data mining;Unsupervised learning;DBSCAN Algorithm;Density Clustering;Machine Learning;Algorithm Research},
  doi={10.1109/IFEEA51475.2020.00199}}

@article{JainEuclideanDistance,
  title={Data clustering: A review},
  author={A. K. Jain and M. N. Murty and P. J. Flynn},
  journal={ACM Computing Surveys},
  volume={31},
  number={3},
  pages={264--323},
  year={1999}
}

@inproceedings{AggarwalManhattanDistance,
  title={Fast Algorithms for Projected Clustering},
  author={C. C. Aggarwal and C. Procopiuc},
  booktitle={Proceedings of the ACM SIGMOD International Conference on Management of Data},
  pages={61--72},
  year={2001}
}

@book{HanMinMaxScaling,
  title={Data Mining: Concepts and Techniques},
  author={J. Han and M. Kamber and J. Pei},
  edition={3rd},
  publisher={Morgan Kaufmann},
  address={Waltham, MA, USA},
  year={2011},
  chapter={Data Preprocessing},
  pages={83--124}
}

@article{AbdiPCAVisualization,
  title={Principal component analysis},
  author={H. Abdi and L. J. Williams},
  journal={Wiley Interdisciplinary Reviews: Computational Statistics},
  volume={2},
  number={4},
  pages={433--459},
  year={2010}
}

@book{GarcíaZ_Score,
  title={Data Mining and Knowledge Discovery Handbook},
  author={S. García and J. Luengo and F. Herrera},
  publisher={Springer},
  year={2015},
  pages={1425--1451},
  chapter={Data Preprocessing in Data Mining}
}

@INPROCEEDINGS{SilhoetterScore,
  author={Shahapure, Ketan Rajshekhar and Nicholas, Charles},
  booktitle={2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA)}, 
  title={Cluster Quality Analysis Using Silhouette Score}, 
  year={2020},
  volume={},
  number={},
  pages={747-748},
  keywords={Clustering algorithms;Iris;Electrical engineering;Computer science;Benchmark testing;Inspection;Writing},
  doi={10.1109/DSAA49011.2020.00096}}


@article{Januzaj_Beqiri_Luma_2023, title={Determining the Optimal Number of Clusters using Silhouette Score as a Data Mining Technique}, volume={19}, url={https://online-journals.org/index.php/i-joe/article/view/37059}, DOI={10.3991/ijoe.v19i04.37059}, abstractNote={&amp;lt;p&amp;gt;The identification of the same objects is very important in determining the similarity between different objects. Nowadays, there are several techniques that allow us to divide objects into different groups that differ from one to another. In order to have the best separation between the clusters, it is required that the optimal determination of the number of clusters of a corpus be made in advance. In our research, the Silhouette score technique was used in order to make the optimal determination of this number of clusters. The application of such a technique was done through the Python language, and a corpus of unstructured job vacancy data was used. After determining the optimal number, at the end we present these clusters and the similarity between them, this presentation will be done in the form of a graph in a suitable format.&amp;lt;/p&amp;gt;}, number={04}, journal={International Journal of Online and Biomedical Engineering (iJOE)}, author={Januzaj, Ylber and Beqiri, Edmond and Luma, Artan}, year={2023}, month={Apr.}, pages={pp. 174–182} }

@article{KodinariyaElbowMethod,
  title={Review on determining number of Cluster in K-Means Clustering},
  author={T. M. Kodinariya and P. R. Makwana},
  journal={International Journal of Advanced Research in Computer Science and Management Studies},
  volume={1},
  number={6},
  pages={90--95},
  year={2013}
}

@inproceedings{HuangCosineRadius,
  title={Similarity measures for text document clustering},
  author={A. Huang},
  booktitle={Proceedings of the New Zealand Computer Science Research Student Conference},
  year={2008}
}



@INPROCEEDINGS{HierarchialClusterSurvey,
  author={Patel, Sakshi and Sihmar, Shivani and Jatain, Aman},
  booktitle={2015 2nd International Conference on Computing for Sustainable Global Development (INDIACom)}, 
  title={A study of hierarchical clustering algorithms}, 
  year={2015},
  volume={},
  number={},
  pages={537-541},
  keywords={Clustering algorithms;Algorithm design and analysis;Heuristic algorithms;Partitioning algorithms;Couplings;Complexity theory;Rocks;Agglomerative;Dendrogram;Hierarchical clusterin},
  doi={}
}

@article{JohnshonHierarchialCluster,
  title={Hierarchical clustering schemes},
  author={S. C. Johnson},
  journal={Psychometrika},
  volume={32},
  number={3},
  pages={241--254},
  year={1967}
}

@article{OyewoleClusteringApplication,
  title={Data clustering: application and trends},
  author={Oyewole, Gbeminiyi John and Thopil, George Alex},
  journal={Artificial Intelligence Review},
  volume={56},
  number={7},
  pages={6439--6475},
  year={2023},
  doi={10.1007/s10462-022-10325-y},
  url={https://doi.org/10.1007/s10462-022-10325-y}
}


@inproceedings{ClusteringMethod,
  title={Some methods for classification and analysis of multivariate observations},
  author={J. MacQueen},
  year={1967},
  url={https://api.semanticscholar.org/CorpusID:6278891}
}

@article{ElectionParticipation,
author = {Partheymüller, Julia and Mueller, Wolfgang C. and Rabitsch, Armin and Lidauer, Michael and Grohma, Paul},
year = {2022},
month = {05},
pages = {},
title = {Participation in the administration of elections and perceptions of electoral integrity},
volume = {77},
journal = {Electoral Studies},
doi = {10.1016/j.electstud.2022.102474}
}

@online{KpuDefinition,
    author    = {Annisa},
    title     = {Komisi Pemilihan Umum (KPU), Tugas dan Wewenangnya},
    date      = {Accessed 2024},
    url       = {Komisi Pemilihan Umum (KPU), Tugas dan Wewenangnya},
}

@inproceedings{KMeansDef,
  title={Some methods for classification and analysis of multivariate observations},
  author={Macqueen, J},
  booktitle={Proceedings of 5-th Berkeley Symposium on Mathematical Statistics and Probability/University of California Press},
  year={1967}
}

@inproceedings{ElbowMethod,
author = {Umargono, Edy and Suseno, Jatmiko and Gunawan, S.K},
year = {2020},
month = {01},
pages = {},
title = {K-Means Clustering Optimization Using the Elbow Method and Early Centroid Determination Based on Mean and Median Formula},
doi = {10.2991/assehr.k.201010.019}
}

@article{DataClusteringAKJein,
author = {Jain, A. K. and Murty, M. N. and Flynn, P. J.},
title = {Data clustering: a review},
year = {1999},
issue_date = {Sept. 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/331499.331504},
doi = {10.1145/331499.331504},
abstract = {Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview
of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify
 cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.},
journal = {ACM Comput. Surv.},
month = sep,
pages = {264–323},
numpages = {60},
keywords = {unsupervised learning, similarity indices, incremental clustering, exploratory data analysis, clustering applications, cluster analysis}
}

@article{ComparationMinMaxZScore,
	author = {Henderi Henderi and Tri Wahyuningsih and Efana Rahwanto},
	title = {Comparison of Min-Max normalization and Z-Score Normalization in the K-nearest neighbor (kNN) Algorithm to Test the Accuracy of Types of Breast Cancer},
	journal = {International Journal of Informatics and Information Systems},
	volume = {4},
	number = {1},
	year = {2021},
	keywords = {K-nearest neighbors; Min-Max Normalization; Z-Score Normalization; Breast Cancer},
	abstract = {The purpose of this study was to examine the results of the prediction of breast cancer, which have been classified based on two types of breast cancer, malignant and benign. The method used in this research is the k-NN algorithm with normalization of min-max and Z-score, the programming language used is the R language. The conclusion is that the highest k accuracy value is k = 5 and k = 21 with an accuracy rate of 98% in the normalization method using the min-max method. Whereas for the Z-score method the highest accuracy is at k = 5 and k = 15 with an accuracy rate of 97%. Thus the min-max normalization method in this study is considered better than the normalization method using the Z-score. The novelty of this research lies in the comparison between the two min-max normalizations and the Z-score normalization in the k-NN algorithm.},
	issn = {2579-7069},	pages = {13--20},	doi = {10.47738/ijiis.v4i1.73},
	url = {https://ijiis.org/index.php/IJIIS/article/view/73}
}

@article{KMeansMethod,
title = {A Clustering Method Based on K-Means Algorithm},
journal = {Physics Procedia},
volume = {25},
pages = {1104-1109},
year = {2012},
note = {International Conference on Solid State Devices and Materials Science, April 1-2, 2012, Macao},
issn = {1875-3892},
doi = {https://doi.org/10.1016/j.phpro.2012.03.206},
url = {https://www.sciencedirect.com/science/article/pii/S1875389212006220},
author = {Youguo Li and Haiyan Wu},
keywords = {cluster analysis, K-Means algorithm, distance algorithm, samples of pattern},
abstract = {In this paper we combine the largest minimum distance algorithm and the traditional K-Means algorithm to propose an improved K-Means clustering algorithm. This improved algorithm can make up the shortcomings for the traditional K-Means algorithm to determine the initial focal point. The improved K-Means algorithm effectively solved two disadvantages of the traditional algorithm, the first one is greater dependence to choice the initial focal point, and another one is easy to be trapped in local minimum[1], [2].}
}

@article{PCAComponent,
  author    = {Michael Greenacre and Patrick J. F. Groenen and Trevor Hastie and Alfonso Iodice D’Enza and Angelos Markos and Elena Tuzhilina},
  title     = {Principal component analysis},
  journal   = {Nature Reviews Methods Primers},
  year      = {2022},
  volume    = {2},
  number    = {1},
  pages     = {100},
  doi       = {10.1038/s43586-022-00184-w},
  url       = {https://doi.org/10.1038/s43586-022-00184-w},
  abstract  = {Principal component analysis is a versatile statistical method for reducing a cases-by-variables data table to its essential features, called principal components. Principal components are a few linear combinations of the original variables that maximally explain the variance of all the variables. In the process, the method provides an approximation of the original data table using only these few major components. This Primer presents a comprehensive review of the method’s definition and geometry, as well as the interpretation of its numerical and graphical results. The main graphical result is often in the form of a biplot, using the major components to map the cases and adding the original variables to support the distance interpretation of the cases’ positions. Variants of the method are also treated, such as the analysis of grouped data, as well as the analysis of categorical data, known as correspondence analysis. Also described and illustrated are the latest innovative applications of principal component analysis: for estimating missing values in huge data matrices, sparse component estimation, and the analysis of images, shapes and functions. Supplementary material includes video animations and computer scripts in the R environment.},
  issn      = {2662-8449}
}

@article{AnalisisDpt,
author = {Fajriansyah, Gilang},
year = {2021},
month = {01},
pages = {39-53},
title = {ANALISIS DAFTAR PEMILIH TETAP PADA HASIL REKAPITULASI KPU BERDASARKAN USIA MENGGUNAKAN ALGORITMA K-MEANS (STUDI KASUS : KOTA BANDAR LAMPUNG)},
volume = {15},
journal = {Electrician},
doi = {10.23960/elc.v15n1.2147}
}

@ARTICLE{FormulaKMeans,
  author={Nie, Feiping and Li, Ziheng and Wang, Rong and Li, Xuelong},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={An Effective and Efficient Algorithm for K-Means Clustering With New Formulation}, 
  year={2023},
  volume={35},
  number={4},
  pages={3433-3443},
  keywords={Clustering algorithms;Machine learning algorithms;Optimization;Convergence;Linear programming;Heuristic algorithms;Costs;Clustering;K-means;optimization;re-weighted},
  doi={10.1109/TKDE.2022.3155450}}
}

@article{AnalisaCluster,
doi = {10.1088/1742-6596/1028/1/012006},
url = {https://dx.doi.org/10.1088/1742-6596/1028/1/012006},
year = {2018},
month = {jun},
publisher = {IOP Publishing},
volume = {1028},
number = {1},
pages = {012006},
author = {Ansari Saleh Ahmar and Darmawan Napitupulu and Robbi Rahim and Rahmat Hidayat and Yance Sonatha and Meri Azmi},
title = {Using K-Means Clustering to Cluster Provinces in Indonesia},
journal = {Journal of Physics: Conference Series},
abstract = {K-Means Clustering (KMC) is a technique used in performing data groupings. The data classification procedure is based on the degree of membership of each member. The purpose of this study is to group the existing Provinces in Indonesia based on Population Density, School Participation Rate, Human Development Index, and Open Unemployment Rate using K-Means Clustering. The result reveals 5 large clusters in each center in South Sumatra, Lampung, DKI Jakarta, Central Java, and West Kalimantan.}
}
